---
title: "code_kindle_text"
author: "Serdar Korur"
date: "12/13/2019"
output: html_document
---

Here is the code related to my medium post at:

https://towardsdatascience.com/an-accidental-side-effect-of-text-mining-4b43f8ee1273

As I read on my kindle I highlight the passages that I like so that I can re-read them later.

These annotations are stored on my Kindle and are backed up at Amazon. And after some time, they started to accumulate and became some kind of data.


I came up with an idea to analyze all those text I highlighted to figure out what kind of content I was most likely to highlight.

The plan was to use text mining and sentiment analysis, generate insights and compare them to my real opinions of those books. So I can have the first-hand test of how useful text mining is. With that knowledge, I can be more convinced when I apply the method to a business problem.


```{r, message=FALSE, warning=FALSE}
# Use readLines function to parse the text file
getwd()
highlights <- readLines("Kindle_highlights_Serdar.Rmd", encoding = "UTF-8")

# Create a dataframe where each row is a line from the text

df <- data.frame(highlights)

# Packages

library(tidyverse)   # includes ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats
library(tidytext)
library(wordcloud2)
```



```{r, message=FALSE, warning=FALSE}
data(stop_words)

# print every 50th word 

stop_words_small <- stop_words[seq(1, nrow(stop_words), 50),]

stop_words_small %>% print(n=50)
```



```{r, message=FALSE, warning=FALSE}
df$highlights <- str_replace_all(df$highlights, "’", "'")
```

Now, the text is ready for the frequency analysis. Words in a text mining project are called tokens. We can split the text into single words by unnest_tokens() function from tidytext package, filter the stop_words and count.

```{r, message=FALSE, warning=FALSE}
df <- df %>% unnest_tokens(word, highlights) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue"))
```



```{r, message=FALSE, warning=FALSE}
top_kindle_highlights <- df %>% 
 group_by(word) %>% 
 count() %>% 
 arrange(desc(n))
```

 **10 most frequent words from my kindle highlights.**

```{r, message=FALSE, warning=FALSE}
top_kindle_highlights
```

Wordclouds are a good alternative to long lists of words for visualizing text data. Wordcloud2 package allows you to use any image as the markup.

```{r, message=FALSE, warning=FALSE, eval=FALSE}
wordcloud2(top_kindle_highlights, figPath = bat, size = 1, backgroundColor = "white", color = color_vector(data$freq) )
```



**Bigram Analysis**  

Single words are a good starting point what the books were about. **But they are not very informative without context.** Frequency analysis can also be performed to measure how often pairs of two words **(bigrams)** occur in the text. This allows us to capture finer details in the text.

  
To do this he combined the unnested single tokens which is isolated above back into a continuous text and then performed bigram analysis. You can use **str_c()** function from stringr package to concatenate the single words.

```{r, message=FALSE, warning=FALSE}
# Recreate the df
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "’", "'")

df <- df %>% unnest_tokens(word, highlights) %>% 
  anti_join(stop_words) %>% 
 filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

df_com <- str_c(df$word, " ") 
df_com <- data.frame(df_com)
```

Let's split the text into bigrams and count the most common two word pairs.

```{r,  message=FALSE, warning=FALSE}
df_bigram <- df_com %>% 
 unnest_tokens(bigram, df_com, token = "ngrams", 
 n = 3, n_min = 2)
top_bigrams <- df_bigram %>% 
 group_by(bigram) %>% 
 count() %>% 
 arrange(desc(n))%>% 
 print(n=20)

# And visualize them on a plot

top <- top_bigrams[1:25,]
```

```{r,  message=FALSE, warning=FALSE}
top %>% ungroup() %>% mutate(bigram = fct_reorder(bigram, n)) %>% 
 ggplot(aes(x=bigram, y=n)) + 
 geom_col() + 
 coord_flip() +
 theme_classic() + 
 theme(legend.position = "none",
 text = element_text(size=18))
```

For example, if you go back above in the top 10 most frequent words table 6th word was change. But we didn't know what the change was about. And here we see that one of the most common bigram is behavioral change. It is making more sense. But it can improve to look at each book individually.

We can also do what we did for the whole document for highlights from single books.

But how can we capture them individually?

Let's first look at the text once more. Before each book **"Your Kindle Notes For:"** appears. 

Let's find out the line numbers for the beginning and the end of each book and use those indexes for fishing out each book.

We will reuse the data frame df we created above. **str_which()** function returns index numbers of the lines which contain a given pattern. In the last step, capturing the text between two consecutive indexes will give us the book between them.

```{r, message=FALSE, warning=FALSE}
# Since I modified df above. I will recreate it again.
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "’", "'")

# Getting the index number for each book

indexes <- str_which(df$highlights, pattern = fixed("Your Kindle Notes For"))
book_names <- df$highlights[indexes + 1]
indexes <-  c(indexes,nrow(df))

# Create an empty list 

books <- list()

# Now the trick. Capture each 28 book separately in a list. 

for(i in 1:(length(indexes)-1)) {
    books[[i]] <- data.frame(df$highlights[(indexes[i]:indexes[i+1]-1)])
    colnames(books[[i]]) <- "word_column"
    books[[i]]$word_column <- as.character(books[[i]]$word_column)
}

```

Let's check whether it worked, for example you can look up the 5th book on our list.

```{r}
head(books[[5]])
head(books[[15]])
```
Now, we have the individual books captured. I will repeat the procedure we used to analyse the whole text above to analyze each of the 28 books by using a for loop.

```{r, message=FALSE, warning=FALSE}
top <- list()
for(i in 1:28){
books[[i]] <- books[[i]] %>% unnest_tokens(word, word_column) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

# Find out the top words in each book and capture them in a list (top)

top[[i]] <- books[[i]] %>% 
              group_by(word) %>% 
              count() %>% 
              arrange(desc(n))
}
for(i in 1:28){
  print(book_names[[i]])
  print(top[[i]])
}

```

Now, looking at the frequent words from each book we can get more insights what they are about.

The bigrams for the same books.

```{r,  message=FALSE, warning=FALSE}
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "’", "'")

# Getting the index number for each book

indexes <- str_which(df$highlights, pattern = fixed("Your Kindle Notes For"))
book_names <- df$highlights[indexes + 1]
indexes <-  c(indexes,nrow(df))

# Capturing each book individually

books <- list()
for (i in 1:(length(indexes)-1)) {
    books[[i]] <- data.frame(df$highlights[(indexes[i]:indexes[i+1]-1)])
    colnames(books[[i]]) <- "word_column"
    books[[i]]$word_column <- as.character(books[[i]]$word_column)
}

# Next step in the plan was splitting the text into single words by unnest_tokens function.



for(i in 1:28){
books[[i]] <- books[[i]] %>% unnest_tokens(word, word_column) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))
}

# After this preparation step I can combine the single words back into a continous text

for(i in 1:28){
books[[i]] <- str_c(books[[i]]$word, " ") 
books[[i]] <- data.frame(books[[i]]) 
}


df_bigram <- list()

for(i in 1:28){                      
df_bigram[[i]] <- books[[i]] %>% 
       unnest_tokens(bigram, books..i.., token = "ngrams", 
                                     n = 3, n_min = 2)
}

for (i in 1:28){
  print(book_names[i])
df_bigram[[i]] %>% 
  group_by(bigram) %>% 
  count() %>% 
  arrange(desc(n))%>% 
  print(n=10)
  
}
```

If you want to see another example of this capturing process you can have a look at my recent post [here](https://dataatomic.com/r/data-wrangling-text-mining/).


Looking at each book individually, he started to be more and more obsessed about the books in my kindle. He decided to order a couple of them.

**Sentiment analysis** is used to evaluate emotional charge in a text mining project. Most common uses are social media monitoring, customer experience management, and Voice of Customer, to understand how they feel.

The **bing** lexicon categorizes words into positive and negative categories, in a binary fashion. The **nrc** lexicon uses categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

**Using bing lexicon**

This gives us the top words contributed to each emotional category. Some examples to note are success, effective, for positive and bad, hard and limits.

Here is how R produced the above plot:

```{r,  message=FALSE, warning=FALSE}
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "’", "'")
df <- df %>% unnest_tokens(word, highlights) %>% 
  anti_join(stop_words) %>% 
 filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

bing_word_counts <- df %>% inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

# Top contributors to positive and negative sentiment


bing <- bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ggplot(aes(reorder(word, n), n, fill=sentiment)) + 
  geom_bar(alpha=0.8, stat="identity", show.legend = FALSE)+
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y= "Contribution to sentiment", x = NULL) +
  coord_flip()
bing
```

**Using nrc lexion**  

We see that I am more likely to highlight if a text is charged with positive rather than negative sentiment, and individually trust, anticipation and joy rather than fear and sadness.

```{r}
df <- data.frame(highlights)
df$highlights <- str_replace_all(df$highlights, "’", "'")
df <- df %>% unnest_tokens(word, highlights) %>% 
  anti_join(stop_words) %>% 
 filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue",
                      "export", "hidden", "truncated", "kindle", "note", "limits"))

sentiment <- df %>%
        left_join(get_sentiments("nrc")) %>%
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = TRUE)
sentiment
```


**Normalized sentiments**

One important thing to add, since each emotion category has different number of words in a language. Emotional categories with less words are less likely to appear in a given text. Thus, I would like to normalize them according to their numbers in the lexicon and see how it differs than the above results.

```{r}
# I will add numbers of each categories from the NRC lexicon

lexicon <- c(2317, 3338, 1234, 842, 1483, 691, 1250, 1195, 1060, 535)
polarity <-  c(1,1,1,1,1,0,0,0,0,0)
sentiment <- data.frame(sentiment, lexicon)
norm_sentiment <- sentiment %>% mutate( normalized = n/lexicon) %>% arrange(desc(normalized))
sentiment <- data.frame(norm_sentiment, polarity)
sentiment

# General findings

sentiment %>% group_by(polarity) %>% summarize(n2 = sum(lexicon))
```

Now, **anticipation** is the highest emotion found in the text that I highlighted. This does not seem a coincidence to me. Since most of the books in our analysis is about productivity and self-development. The productivity tips and tools usually contain words associated with anticipation.  

In a similar way, I can look at the sentiment for individual books

```{r, message=FALSE, warning=FALSE}
df <- data.frame(highlights)

# Kindle uses apostrophes (’), but stop_words uses sigle quotes (') 
# To be able to use all stop_words I should replace apostrophes with quotes
df$highlights <- str_replace_all(df$highlights, "’", "'")

# Getting the index number for each book

indexes <- str_which(df$highlights, pattern = fixed("Your Kindle Notes For"))
book_names <- df$highlights[indexes + 1]
indexes <-  c(indexes,nrow(df))

# Capturing each book individually

books <- list()
for (i in 1:(length(indexes)-1)) {
    books[[i]] <- data.frame(df$highlights[(indexes[i]:indexes[i+1]-1)])
    colnames(books[[i]]) <- "word_column"
    books[[i]]$word_column <- as.character(books[[i]]$word_column)
}

# Next step in the plan was splitting the text into single words by unnest_tokens function.


for(i in 1:28){
books[[i]] <- books[[i]] %>% unnest_tokens(word, word_column) %>%
             anti_join(stop_words) %>% 
             filter(!word %in% c("highlights","highlight", "page", 
                      "location", "yellow", "pink", "orange", "blue"))
}

sentiment <- list()
for (i in 1:28){
sentiment[[i]] <- books[[i]] %>%
        left_join(get_sentiments("nrc")) %>%
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = TRUE)
        print(book_names[i])
        print(sentiment[[i]])
}

for (i in 1:28){
sentiment[[i]] %>% 
    filter(sentiment %in% c('positive','negative')) %>% 
    mutate( n2 = n/sum(n)) %>% print()
  }
```


```{r}
books <- str_trunc(book_names, width=22)
all <- list()
for (i in 1:28) {
all[[i]] <- sentiment[[i]] %>% filter(sentiment %in% c('positive','negative')) %>% mutate(n2 = n/sum(n)) %>% print()
}
```


Positivty Map of the books.

```{r}
all_bound <- do.call("rbind", all) %>% filter(sentiment == "positive")

library(ggrepel)
all_bound %>% ggplot(aes(x= book_names, y=n2)) + 
  geom_point() + 
  geom_label_repel(aes(label=books, color = ifelse(n2 <0.55, "red", "blue")), size = 3) +
  theme_classic() +
  theme(legend.position = "none",
        text = element_text(size=18), 
        axis.text.x = element_blank()) + 
  xlab("Books") + 
  ylab("Positivity score")
```


The lowest positivity score was found in the book **"Man's search for meaning".** This is also kind of expected. Since the book is based on Victor Frankl sufferings during the second world war.

Let's look at the word count in our Outlier. 

```{r}
book_names[[27]]
top[[27]]
```

The word count from the book "The Outliers" below is 107. This is really low. So in the next iteration, I would remove it from the analysis since it will not be very informative. It is hard to know everything from the beginning and we will go back and make some additional cleaning. 

...

### Summary

It is not feasible to read millions of pages to check whether text mining is reliable. But here I got some data that I know the content and I applied text mining approaches and sentiment analysis.
Both the monograms or bigrams pointed to similar ideas what the books were about. And the sentiments made sense with the genres of the books in my kindle.

